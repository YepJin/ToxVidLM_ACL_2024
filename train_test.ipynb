{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e8c787",
   "metadata": {},
   "source": [
    "# Some preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bd3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/ToxVidLM_ACL_2024\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/ToxVidLM_ACL_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38134eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4369ed7",
   "metadata": {},
   "source": [
    "## Packages Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e48f4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.0 (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchvision==0.15.1 (from -r requirements.txt (line 2))\n",
      "  Downloading torchvision-0.15.1-cp38-cp38-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Collecting torchaudio==2.0.1 (from -r requirements.txt (line 3))\n",
      "  Downloading torchaudio-2.0.1-cp38-cp38-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting librosa==0.10.1 (from -r requirements.txt (line 4))\n",
      "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting av==10.0.0 (from -r requirements.txt (line 5))\n",
      "  Downloading av-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opencv-python==4.8.1.78 (from -r requirements.txt (line 6))\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting Pillow==10.0.1 (from -r requirements.txt (line 7))\n",
      "  Downloading Pillow-10.0.1-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting tqdm==4.66.1 (from -r requirements.txt (line 8))\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting numpy==1.24.4 (from -r requirements.txt (line 9))\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting pandas==2.0.3 (from -r requirements.txt (line 10))\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scikit-learn==1.3.2 (from -r requirements.txt (line 11))\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pytorchvideo==0.1.5 (from -r requirements.txt (line 12))\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers==4.35.2 (from -r requirements.txt (line 13))\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Collecting datasets==2.14.6 (from -r requirements.txt (line 14))\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.1 (from -r requirements.txt (line 15))\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting filelock (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.8/site-packages (from torch==2.0.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Collecting sympy (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jinja2 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting requests (from torchvision==0.15.1->-r requirements.txt (line 2))\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting scipy>=1.2.0 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting joblib>=0.14 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.conda/lib/python3.8/site-packages (from librosa==0.10.1->-r requirements.txt (line 4)) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading numba-0.58.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting soundfile>=0.12.1 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.0 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading soxr-0.3.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting lazy-loader>=0.1 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading msgpack-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.8/site-packages (from pandas==2.0.3->-r requirements.txt (line 10)) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas==2.0.3->-r requirements.txt (line 10))\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas==2.0.3->-r requirements.txt (line 10))\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.3.2->-r requirements.txt (line 11))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fvcore (from pytorchvideo==0.1.5->-r requirements.txt (line 12))\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting parameterized (from pytorchvideo==0.1.5->-r requirements.txt (line 12))\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting iopath (from pytorchvideo==0.1.5->-r requirements.txt (line 12))\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2->-r requirements.txt (line 13))\n",
      "  Downloading huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.8/site-packages (from transformers==4.35.2->-r requirements.txt (line 13)) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.35.2->-r requirements.txt (line 13))\n",
      "  Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.35.2->-r requirements.txt (line 13))\n",
      "  Downloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2->-r requirements.txt (line 13))\n",
      "  Downloading tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.35.2->-r requirements.txt (line 13))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting xxhash (from datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading multiprocess-0.70.18-py38-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting responses<0.19 (from evaluate==0.4.1->-r requirements.txt (line 15))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 1)) (75.3.0)\n",
      "Requirement already satisfied: wheel in ./.conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 1)) (0.45.1)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading cmake-4.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r requirements.txt (line 13))\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba>=0.51.0->librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading llvmlite-0.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: importlib-metadata in ./.conda/lib/python3.8/site-packages (from numba>=0.51.0->librosa==0.10.1->-r requirements.txt (line 4)) (8.5.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.conda/lib/python3.8/site-packages (from pooch>=1.0->librosa==0.10.1->-r requirements.txt (line 4)) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3->-r requirements.txt (line 10)) (1.16.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->torchvision==0.15.1->-r requirements.txt (line 2))\n",
      "  Downloading charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision==0.15.1->-r requirements.txt (line 2))\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchvision==0.15.1->-r requirements.txt (line 2))\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision==0.15.1->-r requirements.txt (line 2))\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading cffi-1.17.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo==0.1.5->-r requirements.txt (line 12))\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting termcolor>=1.1 (from fvcore->pytorchvideo==0.1.5->-r requirements.txt (line 12))\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tabulate (from fvcore->pytorchvideo==0.1.5->-r requirements.txt (line 12))\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting portalocker (from iopath->pytorchvideo==0.1.5->-r requirements.txt (line 12))\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading multiprocess-0.70.17-py38-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl.metadata (7.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa==0.10.1->-r requirements.txt (line 4))\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.14.6->-r requirements.txt (line 14))\n",
      "  Downloading propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.conda/lib/python3.8/site-packages (from importlib-metadata->numba>=0.51.0->librosa==0.10.1->-r requirements.txt (line 4)) (3.21.0)\n",
      "Downloading torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.15.1-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.0.1-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "Downloading av-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.0.1-cp38-cp38-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n",
      "Downloading numba-0.58.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.5/746.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.3.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Downloading cffi-1.17.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "Downloading frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading llvmlite-0.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading cmake-4.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188686 sha256=29f5d0a7bef16f4bcb3cdb724c000b405ede5a2296fbf0f90e0d7bfca2562bf8\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/62/e5/0b41f2deb978f449ba3efb4bb24efd6962e4b6abb1fae544ee\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61398 sha256=e272e7923cec1ce37780272c0a81d5267c84099f8b5ab644f00e956e4e520575\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/79/07/c0e9367f5b5ea325e246bd73651e8af175fabbef943043b1cc\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=02ca7234796b98db8b0507c61dc33f76de5e346769d2619daa9802b151dbf9e8\n",
      "  Stored in directory: /root/.cache/pip/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n",
      "Successfully built pytorchvideo fvcore iopath\n",
      "Installing collected packages: pytz, mpmath, lit, av, xxhash, urllib3, tzdata, tqdm, threadpoolctl, termcolor, tabulate, sympy, safetensors, regex, pyyaml, pycparser, propcache, portalocker, Pillow, parameterized, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, multidict, msgpack, MarkupSafe, llvmlite, lazy-loader, joblib, idna, hf-xet, fsspec, frozenlist, filelock, dill, cmake, charset_normalizer, certifi, audioread, attrs, async-timeout, aiohappyeyeballs, yarl, yacs, soxr, scipy, requests, pyarrow, pandas, opencv-python, nvidia-cusolver-cu11, nvidia-cudnn-cu11, numba, multiprocess, jinja2, iopath, cffi, aiosignal, soundfile, scikit-learn, responses, pooch, huggingface-hub, fvcore, aiohttp, tokenizers, pytorchvideo, librosa, transformers, datasets, evaluate, triton, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 Pillow-10.0.1 aiohappyeyeballs-2.4.4 aiohttp-3.10.11 aiosignal-1.3.1 async-timeout-5.0.1 attrs-25.3.0 audioread-3.0.1 av-10.0.0 certifi-2025.7.14 cffi-1.17.1 charset_normalizer-3.4.2 cmake-4.0.3 datasets-2.14.6 dill-0.3.7 evaluate-0.4.1 filelock-3.16.1 frozenlist-1.5.0 fsspec-2023.10.0 fvcore-0.1.5.post20221221 hf-xet-1.1.5 huggingface-hub-0.34.1 idna-3.10 iopath-0.1.10 jinja2-3.1.6 joblib-1.4.2 lazy-loader-0.4 librosa-0.10.1 lit-18.1.8 llvmlite-0.41.1 mpmath-1.3.0 msgpack-1.1.1 multidict-6.1.0 multiprocess-0.70.15 networkx-3.1 numba-0.58.1 numpy-1.24.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 opencv-python-4.8.1.78 pandas-2.0.3 parameterized-0.9.0 pooch-1.8.2 portalocker-3.0.0 propcache-0.2.0 pyarrow-17.0.0 pycparser-2.22 pytorchvideo-0.1.5 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 responses-0.18.0 safetensors-0.5.3 scikit-learn-1.3.2 scipy-1.10.1 soundfile-0.13.1 soxr-0.3.7 sympy-1.13.3 tabulate-0.9.0 termcolor-2.4.0 threadpoolctl-3.5.0 tokenizers-0.15.2 torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1 tqdm-4.66.1 transformers-4.35.2 triton-2.0.0 tzdata-2025.2 urllib3-2.2.3 xxhash-3.5.0 yacs-0.1.8 yarl-1.15.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212194fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.6.2\n",
      "  Downloading peft-0.6.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (25.0)\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (2.0.0)\n",
      "Requirement already satisfied: transformers in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (4.35.2)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (4.66.1)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.6.2)\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in ./.conda/lib/python3.8/site-packages (from peft==0.6.2) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.conda/lib/python3.8/site-packages (from accelerate>=0.21.0->peft==0.6.2) (0.34.1)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.conda/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.6.2) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.6.2) (75.3.0)\n",
      "Requirement already satisfied: wheel in ./.conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.6.2) (0.45.1)\n",
      "Requirement already satisfied: cmake in ./.conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.6.2) (4.0.3)\n",
      "Requirement already satisfied: lit in ./.conda/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.6.2) (18.1.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.8/site-packages (from transformers->peft==0.6.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.8/site-packages (from transformers->peft==0.6.2) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.conda/lib/python3.8/site-packages (from transformers->peft==0.6.2) (0.15.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.21.0->peft==0.6.2) (2023.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.21.0->peft==0.6.2) (1.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.8/site-packages (from jinja2->torch>=1.13.0->peft==0.6.2) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.8/site-packages (from requests->transformers->peft==0.6.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.8/site-packages (from requests->transformers->peft==0.6.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.8/site-packages (from requests->transformers->peft==0.6.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.8/site-packages (from requests->transformers->peft==0.6.2) (2025.7.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/lib/python3.8/site-packages (from sympy->torch>=1.13.0->peft==0.6.2) (1.3.0)\n",
      "Downloading peft-0.6.2-py3-none-any.whl (174 kB)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.0.1 peft-0.6.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5468d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.conda/lib/python3.8/site-packages (from ipywidgets) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: backcall in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pickleshare in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7282c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.0.0\n",
      "Uninstalling torch-2.0.0:\n",
      "  Successfully uninstalled torch-2.0.0\n",
      "Found existing installation: torchvision 0.15.1\n",
      "Uninstalling torchvision-0.15.1:\n",
      "  Successfully uninstalled torchvision-0.15.1\n",
      "Found existing installation: torchaudio 2.0.1\n",
      "Uninstalling torchaudio-2.0.1:\n",
      "  Successfully uninstalled torchaudio-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu102\n",
      "Collecting torch==1.12.1+cu102\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torch-1.12.1%2Bcu102-cp38-cp38-linux_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.13.1+cu102\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torchvision-0.13.1%2Bcu102-cp38-cp38-linux_x86_64.whl (19.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==0.12.1\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torchaudio-0.12.1%2Bcu102-cp38-cp38-linux_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./.conda/lib/python3.8/site-packages (from torch==1.12.1+cu102) (4.12.2)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.8/site-packages (from torchvision==0.13.1+cu102) (1.24.4)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.8/site-packages (from torchvision==0.13.1+cu102) (2.32.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/lib/python3.8/site-packages (from torchvision==0.13.1+cu102) (10.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.8/site-packages (from requests->torchvision==0.13.1+cu102) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.8/site-packages (from requests->torchvision==0.13.1+cu102) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.8/site-packages (from requests->torchvision==0.13.1+cu102) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.8/site-packages (from requests->torchvision==0.13.1+cu102) (2025.7.14)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "peft 0.6.2 requires torch>=1.13.0, but you have torch 1.12.1+cu102 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.1+cu102 torchaudio-0.12.1+cu102 torchvision-0.13.1+cu102\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCUDA available: True\n",
      "CUDA version: 11.7\n",
      "PyTorch version: 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "# Add this cell before the train cell\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "\n",
    "# Verify installation\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77d83f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 10.2\n",
      "PyTorch version: 1.12.1+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a3574",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gdown first\n",
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793bc3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4242053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a public folder, you can try:\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# Create download directory\n",
    "os.makedirs(\"g_downloaded_data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96da0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1Tj31_sm-gLGVtr5ZeqGnmfF_0OxVCRIz classification_final_data-20250630T215505Z-1-001.zip\n",
      "Processing file 1TOHalYk6aZQBOrg1PdPYzaHA0KlobgVX classification_final_data-20250630T215505Z-1-002.zip\n",
      "Processing file 1QhdmWLFPdQFGI3myJ2QpxzT9rXp6cU46 classification_final_data-20250630T215505Z-1-003.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1Tj31_sm-gLGVtr5ZeqGnmfF_0OxVCRIz\n",
      "From (redirected): https://drive.google.com/uc?id=1Tj31_sm-gLGVtr5ZeqGnmfF_0OxVCRIz&confirm=t&uuid=8fefe8f8-f664-40ba-a796-b68dd7f94f8e\n",
      "To: /workspace/g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-001.zip\n",
      "100%|██████████| 2.15G/2.15G [00:44<00:00, 47.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1TOHalYk6aZQBOrg1PdPYzaHA0KlobgVX\n",
      "From (redirected): https://drive.google.com/uc?id=1TOHalYk6aZQBOrg1PdPYzaHA0KlobgVX&confirm=t&uuid=e93b0fa4-a333-4644-b0e6-0f9ca7571115\n",
      "To: /workspace/g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-002.zip\n",
      "100%|██████████| 2.15G/2.15G [00:47<00:00, 45.2MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1QhdmWLFPdQFGI3myJ2QpxzT9rXp6cU46\n",
      "From (redirected): https://drive.google.com/uc?id=1QhdmWLFPdQFGI3myJ2QpxzT9rXp6cU46&confirm=t&uuid=0c1c0d5c-06ed-4278-a650-b99f2b60ef7c\n",
      "To: /workspace/g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-003.zip\n",
      "100%|██████████| 1.65G/1.65G [00:34<00:00, 47.9MB/s]\n",
      "Download completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-001.zip',\n",
       " 'g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-002.zip',\n",
       " 'g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-003.zip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the entire folder (this works for some public folders)\n",
    "folder_id = \"1fddccfJkYZifbRF9zrhYlmX4IAMICnwb\"\n",
    "gdown.download_folder(f\"https://drive.google.com/drive/folders/{folder_id}\", output=\"g_downloaded_data/\", quiet=False, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ab9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-001.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-002.zip...\n",
      "Extracting g_downloaded_data/ToxVid/classification_final_data-20250630T215505Z-1-003.zip...\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "download_dir = \"g_downloaded_data/ToxVid\"\n",
    "# List all files in the download directory\n",
    "for filename in os.listdir(download_dir):\n",
    "    if filename.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(download_dir, filename)\n",
    "        print(f\"Extracting {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_dir)      \n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac605b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ab9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv g_downloaded_data/ToxVid/classification_final_data ToxVidLM_ACL_2024/final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8f0dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/ToxVidLM_ACL_2024\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/ToxVidLM_ACL_2024/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acafed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved successfully.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "source_folder = \"final_data/classification_final_data\"\n",
    "destination_folder = \"final_data\"\n",
    "\n",
    "# Check if source folder exists\n",
    "if os.path.exists(source_folder):\n",
    "    # List all files in the source folder\n",
    "    files = os.listdir(source_folder)\n",
    "    \n",
    "    # Move each file to the destination folder\n",
    "    for file in files:\n",
    "        source_path = os.path.join(source_folder, file)\n",
    "        destination_path = os.path.join(destination_folder, file)\n",
    "        shutil.move(source_path, destination_path)\n",
    "    \n",
    "    print(\"Files moved successfully.\")\n",
    "else:\n",
    "    print(\"Source folder does not exist.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09f3d3",
   "metadata": {},
   "source": [
    "# Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "105064d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/ToxVidLM_ACL_2024\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/ToxVidLM_ACL_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"final_data/\" >> .gitignore\n",
    "!echo \"g_downloaded_data/\" >> .gitignore\n",
    "!echo \"downloaded_data/\" >> .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa13638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'final_data/' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'g_downloaded_data/' did not match any files\n",
      "fatal: pathspec 'downloaded_data/' did not match any files\n"
     ]
    }
   ],
   "source": [
    "# Remove folders from Git tracking (but keep them locally)\n",
    "!git rm -r --cached final_data/\n",
    "!git rm -r --cached g_downloaded_data/\n",
    "!git rm -r --cached downloaded_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9917f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 521de73] Add data folders to .gitignore\n",
      " 3 files changed, 878 insertions(+)\n",
      " create mode 100644 .gitignore\n",
      " delete mode 100644 final_data/final_data.txt.txt\n",
      " create mode 100644 train_test.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Add the .gitignore changes\n",
    "!git add .gitignore\n",
    "\n",
    "# Commit the changes\n",
    "!git commit -m \"Add data folders to .gitignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca2656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email yjin225@wisc.edu\n",
    "!git config --global user.name YepJin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490d636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mmodified:   results/gpt2_vidmae_whisper_sentiment_0.json\u001b[m\n",
      "\t\u001b[32mmodified:   test_tiktok.py\u001b[m\n",
      "\t\u001b[32mmodified:   train_tiktok.py\u001b[m\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mdeleted:    Untitled1.ipynb\u001b[m\n",
      "\t\u001b[31mmodified:   results/gpt2_vidmae_whisper_sentiment_0.json\u001b[m\n",
      "\t\u001b[31mmodified:   test_tiktok.py\u001b[m\n",
      "\t\u001b[31mmodified:   train_test.ipynb\u001b[m\n",
      "\t\u001b[31mmodified:   train_tiktok.py\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mtiktok_data/dataset_450videos_2025stat_update.csv\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c36b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 642cc9d] random states update\n",
      " 11 files changed, 1462 insertions(+), 39 deletions(-)\n",
      " create mode 100644 README_experiments.md\n",
      " create mode 100644 demo_new_features.py\n",
      " delete mode 100644 results/gpt2_vidmae_whisper_offensive_offensive_level_sentiment_0.json\n",
      " create mode 100644 results/gpt2_vidmae_whisper_sentiment_rd1_0.json\n",
      " create mode 100644 results/test_results_rd1.json\n",
      " create mode 100644 run_experiments.py\n",
      " create mode 100644 test_modifications.py\n",
      "Enumerating objects: 19, done.\n",
      "Counting objects: 100% (19/19), done.\n",
      "Delta compression using up to 64 threads\n",
      "Compressing objects: 100% (13/13), done.\n",
      "Writing objects: 100% (13/13), 11.79 KiB | 2.36 MiB/s, done.\n",
      "Total 13 (delta 5), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (5/5), completed with 4 local objects.\u001b[K\n",
      "To https://github.com/YepJin/ToxVidLM_ACL_2024.git\n",
      "   2e229e4..642cc9d  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"random results update\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7529a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92235938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b15cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA memory allocation configuration to reduce fragmentation\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ea3dac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 out of 1\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]Epoch 1 out of 1\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:34<00:00,  3.87s/it]\n",
      "1.0126476221614413\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:34<00:00,  3.87s/it]\n",
      "1.0126476221614413\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:12<00:00,  2.00s/it]\n",
      "sentiment   0.9583333333333334   0.9404761904761904                 precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.96      1.00      0.98        23\n",
      "\n",
      "    accuracy                           0.96        24\n",
      "   macro avg       0.48      0.50      0.49        24\n",
      "weighted avg       0.92      0.96      0.94        24\n",
      " \n",
      "\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:12<00:00,  2.00s/it]\n",
      "sentiment   0.9583333333333334   0.9404761904761904                 precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.96      1.00      0.98        23\n",
      "\n",
      "    accuracy                           0.96        24\n",
      "   macro avg       0.48      0.50      0.49        24\n",
      "weighted avg       0.92      0.96      0.94        24\n",
      " \n",
      "\n",
      "saved best model with metric:  f1  for task:  sentiment\n",
      "saved best model with metric:  f1  for task:  sentiment\n",
      "Training finished!\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0505f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "load state dict checkpoints/gpt2_vidmae_whisper_sentiment_0.pth\n",
      "load state dict checkpoints/gpt2_vidmae_whisper_sentiment_0.pth\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:07<00:00,  2.56s/it]\n",
      "sentiment   1.0   1.0                 precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      " \n",
      "\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:07<00:00,  2.56s/it]\n",
      "sentiment   1.0   1.0                 precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a66d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change it to 5-level and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24b3bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"final_data/final_processed_data_one_hot.csv\")\n",
    "df2= pd.read_csv(\"final_data/final_processed_data_one_hot_5_levels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22833f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf2edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     video_no  sentence_no split_video_no  \\\n",
      "0           0  video_1.mp4            1  video_1_1.mp4   \n",
      "1           1  video_1.mp4            2  video_1_2.mp4   \n",
      "2           2  video_1.mp4            3  video_1_3.mp4   \n",
      "3           3  video_1.mp4            4  video_1_4.mp4   \n",
      "4           4  video_1.mp4            5  video_1_5.mp4   \n",
      "\n",
      "                                                text start_time end_time  \\\n",
      "0            what are your strengths and weaknesses?      0.227    3.903   \n",
      "1             sir, yeh sabse chutiya sawaal hai, sir      4.079    5.651   \n",
      "2                sabse pehle toh strength hai hi nhi      5.906    7.654   \n",
      "3                                 in english, please      8.154    9.655   \n",
      "4  I'm telling you. Why will I tell you my weakne...      9.656   13.651   \n",
      "\n",
      "    offensive offensiveness level                  sentiment  \\\n",
      "0  [1.0, 0.0]     [1.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
      "1  [0.0, 1.0]     [0.0, 1.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
      "2  [1.0, 0.0]     [1.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
      "3  [1.0, 0.0]     [1.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
      "4  [1.0, 0.0]     [1.0, 0.0, 0.0]  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
      "\n",
      "                          video_path                             audio_path  \n",
      "0  ./final_data/videos/video_1_1.mp4  ./final_data/audio_conv/video_1_1.npy  \n",
      "1  ./final_data/videos/video_1_2.mp4  ./final_data/audio_conv/video_1_2.npy  \n",
      "2  ./final_data/videos/video_1_3.mp4  ./final_data/audio_conv/video_1_3.npy  \n",
      "3  ./final_data/videos/video_1_4.mp4  ./final_data/audio_conv/video_1_4.npy  \n",
      "4  ./final_data/videos/video_1_5.mp4  ./final_data/audio_conv/video_1_5.npy  \n"
     ]
    }
   ],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65619d",
   "metadata": {},
   "source": [
    "## Audio and Video Inputs for train_tiktok.py\n",
    "\n",
    "Based on the code analysis, here's what the audio and video inputs are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41d503eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIO AND VIDEO INPUTS ANALYSIS\n",
      "==================================================\n",
      "1. DATA SOURCE:\n",
      "   File: final_data/final_processed_data_one_hot_5_levels.csv\n",
      "   Contains paths to video and audio files\n",
      "\n",
      "2. DATASET INFO:\n",
      "   Total samples: 4021\n",
      "   Columns: ['Unnamed: 0', 'video_no', 'sentence_no', 'split_video_no', 'text', 'start_time', 'end_time', 'offensive', 'offensiveness level', 'sentiment', 'video_path', 'audio_path']\n",
      "\n",
      "3. SAMPLE PATHS:\n",
      "   Sample 1:\n",
      "     Text: 'what are your strengths and weaknesses?'\n",
      "     Video: ./final_data/videos/video_1_1.mp4\n",
      "     Audio: ./final_data/audio_conv/video_1_1.npy\n",
      "     Video exists: True\n",
      "     Audio exists: True\n",
      "\n",
      "   Sample 2:\n",
      "     Text: 'sir, yeh sabse chutiya sawaal hai, sir'\n",
      "     Video: ./final_data/videos/video_1_2.mp4\n",
      "     Audio: ./final_data/audio_conv/video_1_2.npy\n",
      "     Video exists: True\n",
      "     Audio exists: True\n",
      "\n",
      "   Sample 3:\n",
      "     Text: 'sabse pehle toh strength hai hi nhi'\n",
      "     Video: ./final_data/videos/video_1_3.mp4\n",
      "     Audio: ./final_data/audio_conv/video_1_3.npy\n",
      "     Video exists: True\n",
      "     Audio exists: True\n",
      "\n",
      "4. PROCESSING PIPELINE:\n",
      "   📹 VIDEO:\n",
      "      - Format: .mp4 files\n",
      "      - Location: ./final_data/videos/\n",
      "      - Processing: VideoMAE processor (MCG-NJU/videomae-base)\n",
      "      - Output shape: [16, 3, 224, 224] (16 frames, 3 channels, 224x224 pixels)\n",
      "      - Frame sampling: 16 frames extracted from each video clip\n",
      "\n",
      "   🎵 AUDIO:\n",
      "      - Format: .npy files (preprocessed audio features)\n",
      "      - Location: ./final_data/audio_conv/\n",
      "      - Processing: Whisper feature extractor (openai/whisper-small)\n",
      "      - Output: Whisper input features for audio understanding\n",
      "      - Contains: Audio features extracted from the video segments\n",
      "\n",
      "5. MULTIMODAL FUSION:\n",
      "   - Text (dialogue) + Video (visual) + Audio (speech) → Sentiment Classification\n",
      "   - All three modalities are processed and fused in the model\n",
      "   - Target: 5-level sentiment classification\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the audio and video inputs for train_tiktok.py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"AUDIO AND VIDEO INPUTS ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Data source\n",
    "print(\"1. DATA SOURCE:\")\n",
    "print(\"   File: final_data/final_processed_data_one_hot_5_levels.csv\")\n",
    "print(\"   Contains paths to video and audio files\")\n",
    "\n",
    "# 2. Check a few samples\n",
    "df = pd.read_csv('final_data/final_processed_data_one_hot_5_levels.csv')\n",
    "print(f\"\\n2. DATASET INFO:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "print(f\"\\n3. SAMPLE PATHS:\")\n",
    "for i in range(3):\n",
    "    video_path = df['video_path'].iloc[i]\n",
    "    audio_path = df['audio_path'].iloc[i]\n",
    "    text = df['text'].iloc[i]\n",
    "    print(f\"   Sample {i+1}:\")\n",
    "    print(f\"     Text: '{text}'\")\n",
    "    print(f\"     Video: {video_path}\")\n",
    "    print(f\"     Audio: {audio_path}\")\n",
    "    print(f\"     Video exists: {os.path.exists(video_path)}\")\n",
    "    print(f\"     Audio exists: {os.path.exists(audio_path)}\")\n",
    "    print()\n",
    "\n",
    "print(\"4. PROCESSING PIPELINE:\")\n",
    "print(\"   📹 VIDEO:\")\n",
    "print(\"      - Format: .mp4 files\")\n",
    "print(\"      - Location: ./final_data/videos/\")\n",
    "print(\"      - Processing: VideoMAE processor (MCG-NJU/videomae-base)\")\n",
    "print(\"      - Output shape: [16, 3, 224, 224] (16 frames, 3 channels, 224x224 pixels)\")\n",
    "print(\"      - Frame sampling: 16 frames extracted from each video clip\")\n",
    "print()\n",
    "print(\"   🎵 AUDIO:\")\n",
    "print(\"      - Format: .npy files (preprocessed audio features)\")\n",
    "print(\"      - Location: ./final_data/audio_conv/\") \n",
    "print(\"      - Processing: Whisper feature extractor (openai/whisper-small)\")\n",
    "print(\"      - Output: Whisper input features for audio understanding\")\n",
    "print(\"      - Contains: Audio features extracted from the video segments\")\n",
    "\n",
    "print(\"\\n5. MULTIMODAL FUSION:\")\n",
    "print(\"   - Text (dialogue) + Video (visual) + Audio (speech) → Sentiment Classification\")\n",
    "print(\"   - All three modalities are processed and fused in the model\")\n",
    "print(\"   - Target: 5-level sentiment classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08da53fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL DATA SHAPES AND FORMATS\n",
      "========================================\n",
      "Sample: 'what are your strengths and weaknesses?'\n",
      "Video path: ./final_data/videos/video_1_1.mp4\n",
      "Audio path: ./final_data/audio_conv/video_1_1.npy\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/lib/python3.8/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📹 VIDEO TENSOR:\n",
      "   Shape: torch.Size([16, 3, 224, 224])\n",
      "   Data type: torch.float32\n",
      "   Min value: -2.118\n",
      "   Max value: 1.769\n",
      "   Memory size: 9.19 MB\n",
      "\n",
      "🎵 AUDIO TENSOR:\n",
      "   Shape: torch.Size([80, 3000])\n",
      "   Data type: torch.float32\n",
      "   Min value: -0.614\n",
      "   Max value: 1.386\n",
      "   Memory size: 0.92 MB\n",
      "\n",
      "🎧 RAW AUDIO DATA (before processing):\n",
      "   Shape: (59125,)\n",
      "   Data type: float64\n",
      "   Duration: ~3.70 seconds (assuming 16kHz)\n",
      "\n",
      "✅ All data loaded successfully!\n",
      "\n",
      "📊 INPUT SUMMARY for train_tiktok.py:\n",
      "   • Text: 'what are your strengths and weaknesses?' (tokenized to max 61 tokens)\n",
      "   • Video: torch.Size([16, 3, 224, 224]) tensor (16 frames of 224x224 RGB)\n",
      "   • Audio: torch.Size([80, 3000]) tensor (Whisper features)\n",
      "   • Labels: 5-level sentiment one-hot vector\n"
     ]
    }
   ],
   "source": [
    "# Let's load and examine the actual data shapes\n",
    "from data.utils import return_audio_tensor, return_video_tensor\n",
    "import numpy as np\n",
    "\n",
    "print(\"ACTUAL DATA SHAPES AND FORMATS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load sample data\n",
    "sample_video_path = df['video_path'].iloc[0]\n",
    "sample_audio_path = df['audio_path'].iloc[0]\n",
    "sample_text = df['text'].iloc[0]\n",
    "\n",
    "print(f\"Sample: '{sample_text}'\")\n",
    "print(f\"Video path: {sample_video_path}\")\n",
    "print(f\"Audio path: {sample_audio_path}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Load video\n",
    "    video_tensor = return_video_tensor(sample_video_path)\n",
    "    print(f\"📹 VIDEO TENSOR:\")\n",
    "    print(f\"   Shape: {video_tensor.shape}\")\n",
    "    print(f\"   Data type: {video_tensor.dtype}\")\n",
    "    print(f\"   Min value: {video_tensor.min():.3f}\")\n",
    "    print(f\"   Max value: {video_tensor.max():.3f}\")\n",
    "    print(f\"   Memory size: {video_tensor.element_size() * video_tensor.nelement() / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Load audio\n",
    "    audio_tensor = return_audio_tensor(sample_audio_path)\n",
    "    print(f\"\\n🎵 AUDIO TENSOR:\")\n",
    "    print(f\"   Shape: {audio_tensor.shape}\")\n",
    "    print(f\"   Data type: {audio_tensor.dtype}\")\n",
    "    print(f\"   Min value: {audio_tensor.min():.3f}\")\n",
    "    print(f\"   Max value: {audio_tensor.max():.3f}\")\n",
    "    print(f\"   Memory size: {audio_tensor.element_size() * audio_tensor.nelement() / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Check raw audio data\n",
    "    raw_audio = np.load(sample_audio_path)\n",
    "    print(f\"\\n🎧 RAW AUDIO DATA (before processing):\")\n",
    "    print(f\"   Shape: {raw_audio.shape}\")\n",
    "    print(f\"   Data type: {raw_audio.dtype}\")\n",
    "    print(f\"   Duration: ~{len(raw_audio)/16000:.2f} seconds (assuming 16kHz)\")\n",
    "    \n",
    "    print(f\"\\n✅ All data loaded successfully!\")\n",
    "    print(f\"\\n📊 INPUT SUMMARY for train_tiktok.py:\")\n",
    "    print(f\"   • Text: '{sample_text}' (tokenized to max 61 tokens)\")\n",
    "    print(f\"   • Video: {video_tensor.shape} tensor (16 frames of 224x224 RGB)\")\n",
    "    print(f\"   • Audio: {audio_tensor.shape} tensor (Whisper features)\")\n",
    "    print(f\"   • Labels: 5-level sentiment one-hot vector\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    print(\"Make sure the video and audio files exist in the specified paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8271a",
   "metadata": {},
   "source": [
    "## Converting MP4 to .npy Audio Files\n",
    "\n",
    "Let's demonstrate how to extract audio from MP4 files and convert them to the .npy format used in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6912012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required libraries...\n",
      "✅ librosa is already installed\n",
      "✅ ffmpeg is available\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install required libraries for audio processing\n",
    "print(\"Installing required libraries...\")\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if librosa is installed\n",
    "try:\n",
    "    import librosa\n",
    "    print(\"✅ librosa is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing librosa...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'librosa'])\n",
    "    import librosa\n",
    "    print(\"✅ librosa installed successfully\")\n",
    "\n",
    "# Check if ffmpeg is available\n",
    "try:\n",
    "    subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)\n",
    "    print(\"✅ ffmpeg is available\")\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    print(\"⚠️  ffmpeg not found. Installing...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ffmpeg-python'])\n",
    "    print(\"✅ ffmpeg-python installed\")\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13679a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIO EXTRACTION EXAMPLE\n",
      "==================================================\n",
      "Input MP4: ./final_data/videos/video_1_1.mp4\n",
      "Output NPY: ./final_data/audio_conv/video_1_1_extracted.npy\n",
      "MP4 exists: True\n",
      "\n",
      "🎬 Processing: ./final_data/videos/video_1_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75473/2534357755.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
      "/workspace/.conda/lib/python3.8/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 Original sampling rate: 16000 Hz\n",
      "   📊 Audio shape: (59126,)\n",
      "   📊 Duration: 3.70 seconds\n",
      "   📊 Data type: float32\n",
      "   📊 Value range: [-0.990, 1.002]\n",
      "   ✅ Saved to: ./final_data/audio_conv/video_1_1_extracted.npy\n",
      "   ✅ Verification: Loaded shape (59126,)\n",
      "\n",
      "🎉 SUCCESS! Audio extracted and saved as .npy file\n",
      "\n",
      "📊 COMPARISON:\n",
      "   Extracted audio shape: (59126,)\n",
      "   Existing audio shape:  (59125,)\n",
      "   Shapes match: False\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Function to extract audio from MP4 and save as .npy\n",
    "def extract_audio_to_npy(mp4_path, output_npy_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Extract audio from MP4 file and save as .npy format\n",
    "    \n",
    "    Args:\n",
    "        mp4_path (str): Path to the MP4 file\n",
    "        output_npy_path (str): Path where to save the .npy file\n",
    "        target_sr (int): Target sampling rate (16kHz for Whisper)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🎬 Processing: {mp4_path}\")\n",
    "        \n",
    "        # Load audio from MP4 using librosa\n",
    "        # This automatically converts to mono and resamples to target_sr\n",
    "        audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
    "        \n",
    "        print(f\"   📊 Original sampling rate: {sr} Hz\")\n",
    "        print(f\"   📊 Audio shape: {audio_data.shape}\")\n",
    "        print(f\"   📊 Duration: {len(audio_data)/sr:.2f} seconds\")\n",
    "        print(f\"   📊 Data type: {audio_data.dtype}\")\n",
    "        print(f\"   📊 Value range: [{audio_data.min():.3f}, {audio_data.max():.3f}]\")\n",
    "        \n",
    "        # Save as numpy array\n",
    "        np.save(output_npy_path, audio_data)\n",
    "        print(f\"   ✅ Saved to: {output_npy_path}\")\n",
    "        \n",
    "        # Verify the saved file\n",
    "        loaded_audio = np.load(output_npy_path)\n",
    "        print(f\"   ✅ Verification: Loaded shape {loaded_audio.shape}\")\n",
    "        \n",
    "        return audio_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error processing {mp4_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 3: Example usage with video_1_1.mp4\n",
    "mp4_file = \"./final_data/videos/video_1_1.mp4\"\n",
    "npy_output = \"./final_data/audio_conv/video_1_1_extracted.npy\"\n",
    "\n",
    "print(\"AUDIO EXTRACTION EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input MP4: {mp4_file}\")\n",
    "print(f\"Output NPY: {npy_output}\")\n",
    "print(f\"MP4 exists: {os.path.exists(mp4_file)}\")\n",
    "print()\n",
    "\n",
    "if os.path.exists(mp4_file):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(npy_output), exist_ok=True)\n",
    "    \n",
    "    # Extract audio\n",
    "    extracted_audio = extract_audio_to_npy(mp4_file, npy_output)\n",
    "    \n",
    "    if extracted_audio is not None:\n",
    "        print(f\"\\n🎉 SUCCESS! Audio extracted and saved as .npy file\")\n",
    "        \n",
    "        # Compare with existing .npy file if available\n",
    "        existing_npy = \"./final_data/audio_conv/video_1_1.npy\"\n",
    "        if os.path.exists(existing_npy):\n",
    "            existing_audio = np.load(existing_npy)\n",
    "            print(f\"\\n📊 COMPARISON:\")\n",
    "            print(f\"   Extracted audio shape: {extracted_audio.shape}\")\n",
    "            print(f\"   Existing audio shape:  {existing_audio.shape}\")\n",
    "            print(f\"   Shapes match: {extracted_audio.shape == existing_audio.shape}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ MP4 file not found: {mp4_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6c14ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 BATCH PROCESSING\n",
      "Input directory: ./final_data/videos/\n",
      "Output directory: ./final_data/audio_conv_new/\n",
      "==================================================\n",
      "Found 4022 MP4 files\n",
      "\n",
      "📹 Processing 1/5: video_9_3.mp4\n",
      "🎬 Processing: ./final_data/videos/video_9_3.mp4\n",
      "   📊 Original sampling rate: 16000 Hz\n",
      "   📊 Audio shape: (28512,)\n",
      "   📊 Duration: 1.78 seconds\n",
      "   📊 Data type: float32\n",
      "   📊 Value range: [-0.228, 0.218]\n",
      "   ✅ Saved to: ./final_data/audio_conv_new/video_9_3.npy\n",
      "   ✅ Verification: Loaded shape (28512,)\n",
      "\n",
      "📹 Processing 2/5: video_96_1.mp4\n",
      "🎬 Processing: ./final_data/videos/video_96_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75473/2534357755.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
      "/workspace/.conda/lib/python3.8/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "/tmp/ipykernel_75473/2534357755.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
      "/workspace/.conda/lib/python3.8/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 Original sampling rate: 16000 Hz\n",
      "   📊 Audio shape: (48886,)\n",
      "   📊 Duration: 3.06 seconds\n",
      "   📊 Data type: float32\n",
      "   📊 Value range: [-0.020, 0.015]\n",
      "   ✅ Saved to: ./final_data/audio_conv_new/video_96_1.npy\n",
      "   ✅ Verification: Loaded shape (48886,)\n",
      "\n",
      "📹 Processing 3/5: video_9_1.mp4\n",
      "🎬 Processing: ./final_data/videos/video_9_1.mp4\n",
      "   📊 Original sampling rate: 16000 Hz\n",
      "   📊 Audio shape: (30080,)\n",
      "   📊 Duration: 1.88 seconds\n",
      "   📊 Data type: float32\n",
      "   📊 Value range: [-0.201, 0.191]\n",
      "   ✅ Saved to: ./final_data/audio_conv_new/video_9_1.npy\n",
      "   ✅ Verification: Loaded shape (30080,)\n",
      "\n",
      "📹 Processing 4/5: video_96_6.mp4\n",
      "🎬 Processing: ./final_data/videos/video_96_6.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75473/2534357755.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
      "/workspace/.conda/lib/python3.8/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "/tmp/ipykernel_75473/2534357755.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
      "/workspace/.conda/lib/python3.8/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📊 Original sampling rate: 16000 Hz\n",
      "   📊 Audio shape: (63659,)\n",
      "   📊 Duration: 3.98 seconds\n",
      "   📊 Data type: float32\n",
      "   📊 Value range: [-0.117, 0.170]\n",
      "   ✅ Saved to: ./final_data/audio_conv_new/video_96_6.npy\n",
      "   ✅ Verification: Loaded shape (63659,)\n",
      "\n",
      "📹 Processing 5/5: video_97_4.mp4\n",
      "🎬 Processing: ./final_data/videos/video_97_4.mp4\n",
      "   📊 Original sampling rate: 16000 Hz\n",
      "   📊 Audio shape: (20011,)\n",
      "   📊 Duration: 1.25 seconds\n",
      "   📊 Data type: float32\n",
      "   📊 Value range: [-0.172, 0.187]\n",
      "   ✅ Saved to: ./final_data/audio_conv_new/video_97_4.npy\n",
      "   ✅ Verification: Loaded shape (20011,)\n",
      "\n",
      "✅ Batch processing complete: 5/5 files processed successfully\n",
      "\n",
      "============================================================\n",
      "📝 EXPLANATION OF THE PROCESS:\n",
      "============================================================\n",
      "1. 🎬 MP4 Input: Original video file with audio track\n",
      "2. 🔊 Audio Extraction: Uses librosa to extract audio data\n",
      "3. 📊 Preprocessing:\n",
      "   • Convert to mono (single channel)\n",
      "   • Resample to 16kHz (Whisper's expected sample rate)\n",
      "   • Normalize audio values to [-1, 1] range\n",
      "4. 💾 Save as .npy: Store as NumPy array for fast loading\n",
      "5. 🔮 Model Usage: Whisper feature extractor processes this audio\n",
      "\n",
      "⚠️  NOTE: Small differences in audio length (59125 vs 59126 samples)\n",
      "   are normal due to different audio processing pipelines.\n",
      "   The model can handle these slight variations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75473/2534357755.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
      "/workspace/.conda/lib/python3.8/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Batch processing function for multiple MP4 files\n",
    "def batch_extract_audio(video_dir, audio_output_dir, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Extract audio from all MP4 files in a directory\n",
    "    \n",
    "    Args:\n",
    "        video_dir (str): Directory containing MP4 files\n",
    "        audio_output_dir (str): Directory to save .npy files\n",
    "        target_sr (int): Target sampling rate\n",
    "    \"\"\"\n",
    "    print(f\"🔄 BATCH PROCESSING\")\n",
    "    print(f\"Input directory: {video_dir}\")\n",
    "    print(f\"Output directory: {audio_output_dir}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(audio_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all MP4 files\n",
    "    mp4_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "    print(f\"Found {len(mp4_files)} MP4 files\")\n",
    "    \n",
    "    success_count = 0\n",
    "    for i, mp4_file in enumerate(mp4_files[:5]):  # Process first 5 as example\n",
    "        print(f\"\\n📹 Processing {i+1}/{min(5, len(mp4_files))}: {mp4_file}\")\n",
    "        \n",
    "        mp4_path = os.path.join(video_dir, mp4_file)\n",
    "        npy_name = mp4_file.replace('.mp4', '.npy')\n",
    "        npy_path = os.path.join(audio_output_dir, npy_name)\n",
    "        \n",
    "        audio = extract_audio_to_npy(mp4_path, npy_path, target_sr)\n",
    "        if audio is not None:\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"\\n✅ Batch processing complete: {success_count}/{min(5, len(mp4_files))} files processed successfully\")\n",
    "\n",
    "# Example: Process first 5 MP4 files\n",
    "video_directory = \"./final_data/videos/\"\n",
    "audio_output_directory = \"./final_data/audio_conv_new/\"\n",
    "\n",
    "if os.path.exists(video_directory):\n",
    "    batch_extract_audio(video_directory, audio_output_directory)\n",
    "else:\n",
    "    print(f\"❌ Video directory not found: {video_directory}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📝 EXPLANATION OF THE PROCESS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. 🎬 MP4 Input: Original video file with audio track\")\n",
    "print(\"2. 🔊 Audio Extraction: Uses librosa to extract audio data\")\n",
    "print(\"3. 📊 Preprocessing:\")\n",
    "print(\"   • Convert to mono (single channel)\")\n",
    "print(\"   • Resample to 16kHz (Whisper's expected sample rate)\")\n",
    "print(\"   • Normalize audio values to [-1, 1] range\")\n",
    "print(\"4. 💾 Save as .npy: Store as NumPy array for fast loading\")\n",
    "print(\"5. 🔮 Model Usage: Whisper feature extractor processes this audio\")\n",
    "print()\n",
    "print(\"⚠️  NOTE: Small differences in audio length (59125 vs 59126 samples)\")\n",
    "print(\"   are normal due to different audio processing pipelines.\")\n",
    "print(\"   The model can handle these slight variations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899956a",
   "metadata": {},
   "source": [
    "# Deal with our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "509e3ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8802c9",
   "metadata": {},
   "source": [
    "## Download tiktok videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ed170ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a public folder, you can try:\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# Create download directory\n",
    "os.makedirs(\"g_video\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7c42e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1e8Z6S5hymxOAxXhAtrplsxwR-ahTecVS\n",
      "From (redirected): https://drive.google.com/uc?id=1e8Z6S5hymxOAxXhAtrplsxwR-ahTecVS&confirm=t&uuid=4064462c-f540-4b92-bd6a-33b4bcc7592e\n",
      "To: /workspace/g_video/downloaded_file.zip\n",
      "100%|██████████| 2.52G/2.52G [05:41<00:00, 7.39MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'g_video/downloaded_file.zip'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Direct download using the file ID\n",
    "file_id = \"1e8Z6S5hymxOAxXhAtrplsxwR-ahTecVS\"\n",
    "output_path = \"g_video/downloaded_file.zip\"\n",
    "\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294c07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting g_video/downloaded_file.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "download_dir = \"g_video\"\n",
    "# List all files in the download directory\n",
    "for filename in os.listdir(download_dir):\n",
    "    if filename.endswith(\".zip\") and filename==\"downloaded_file.zip\":\n",
    "        zip_path = os.path.join(download_dir, filename)\n",
    "        print(f\"Extracting {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_dir)      \n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba9a2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t   checkpoints\t      model\t\ttrain.py\n",
      "README.md\t   data\t\t      requirements.txt\ttrain_test.ipynb\n",
      "ToxVidLM_ACL_2024  final_data\t      results\t\ttrain_tiktok.py\n",
      "Untitled1.ipynb    g_downloaded_data  test.py\n",
      "__pycache__\t   iteration.py       tiktok_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "source_folder = \"g_video/450audios\"\n",
    "destination_folder = \"ToxVidLM_ACL_2024/tiktok_data/audio_conv\"\n",
    "\n",
    "source_folder = \"g_video/450videos\"\n",
    "destination_folder = \"ToxVidLM_ACL_2024/tiktok_data/video\"\n",
    "\n",
    "# Create destination directory if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "files = os.listdir(source_folder)\n",
    "print(f\"Found {len(files)} files to copy\")\n",
    "\n",
    "for file in files:\n",
    "    source_path = os.path.join(source_folder, file)\n",
    "    destination_path = os.path.join(destination_folder, file)\n",
    "    \n",
    "    if os.path.isfile(source_path):\n",
    "        shutil.copy2(source_path, destination_path)\n",
    "        print(f\"Copied: {file}\")\n",
    "\n",
    "print(f\"Successfully copied {len(files)} files from {source_folder} to {destination_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec2fd6",
   "metadata": {},
   "source": [
    "### Convert mp4 to audio.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19479e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ffmpeg is available\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import librosa\n",
    "subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)\n",
    "print(\"✅ ffmpeg is available\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100f5ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 Processing: ./g_video/450videos/v_6791863658646490373.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_331300/4175797287.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
      "/workspace/.conda/lib/python3.8/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Saved to: ./g_video/450audios/v_6791863658646490373.npy\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Function to extract audio from MP4 and save as .npy\n",
    "def extract_audio_to_npy(mp4_path, output_npy_path, target_sr=16000):\n",
    "\n",
    "        print(f\"🎬 Processing: {mp4_path}\")\n",
    "        audio_data, sr = librosa.load(mp4_path, sr=target_sr, mono=True)\n",
    "        np.save(output_npy_path, audio_data)\n",
    "        print(f\"   ✅ Saved to: {output_npy_path}\")\n",
    "        return audio_data\n",
    "        \n",
    "\n",
    "# Step 3: Example usage with video_1_1.mp4\n",
    "mp4_file = \"./g_video/450videos/v_6791863658646490373.mp4\"\n",
    "npy_output = \"./g_video/450audios/v_6791863658646490373.npy\"\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(mp4_file):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(npy_output), exist_ok=True)\n",
    "    \n",
    "    extracted_audio = extract_audio_to_npy(mp4_file, npy_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653f188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Batch processing function for multiple MP4 files\n",
    "def batch_extract_audio(video_dir, audio_output_dir, target_sr=16000):\n",
    "\n",
    "    os.makedirs(audio_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all MP4 files\n",
    "    mp4_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "    print(f\"Found {len(mp4_files)} MP4 files\")\n",
    "    \n",
    "    success_count = 0\n",
    "    for i, mp4_file in enumerate(mp4_files):  # Process first 5 as example\n",
    "        print(f\"\\n📹 Processing {i+1}/{min(5, len(mp4_files))}: {mp4_file}\")\n",
    "        \n",
    "        mp4_path = os.path.join(video_dir, mp4_file)\n",
    "        npy_name = mp4_file.replace('.mp4', '.npy')\n",
    "        npy_path = os.path.join(audio_output_dir, npy_name)\n",
    "        \n",
    "        audio = extract_audio_to_npy(mp4_path, npy_path, target_sr)\n",
    "        if audio is not None:\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"\\n✅ Batch processing complete: {success_count}/{min(5, len(mp4_files))} files processed successfully\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process first 5 MP4 files\n",
    "\n",
    "\n",
    "video_directory = \"./g_video/450videos\"\n",
    "audio_output_directory = \"./g_video/450audios/\"\n",
    "\n",
    "if os.path.exists(video_directory):\n",
    "    batch_extract_audio(video_directory, audio_output_directory)\n",
    "else:\n",
    "    print(f\"❌ Video directory not found: {video_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1366cd6",
   "metadata": {},
   "source": [
    "## Convert our dataset to the Toxvid format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26113c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv(\"ToxVidLM_ACL_2024/final_data/final_processed_data_one_hot_5_levels.csv\")\n",
    "df2=pd.read_csv(\"ToxVidLM_ACL_2024/tiktok_data/video_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebc0628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "1    2\n",
      "2    2\n",
      "3    3\n",
      "4    5\n",
      "Name: rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"rating\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b499d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "1    [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "2    [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "3    [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "4    [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df1[\"sentiment\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "258a3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the conversion function:\n",
      "Rating 1 -> [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Rating 2 -> [0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Rating 3 -> [0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "Rating 4 -> [0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "Rating 5 -> [0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def rating_to_one_hot(rating, num_classes=5):\n",
    "\n",
    "    # Create one-hot vector\n",
    "    one_hot = [0.0] * num_classes\n",
    "    \n",
    "    # Set the appropriate index to 1.0 (rating-1 because ratings are 1-indexed)\n",
    "    if 1 <= rating <= num_classes:\n",
    "        one_hot[rating - 1] = 1.0\n",
    "    else:\n",
    "        print(f\"Warning: Rating {rating} is out of range [1, {num_classes}]\")\n",
    "    \n",
    "    # Return as string representation (to match df1[\"sentiment\"] format)\n",
    "    return str(one_hot)\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing the conversion function:\")\n",
    "for rating in [1, 2, 3, 4, 5]:\n",
    "    converted = rating_to_one_hot(rating)\n",
    "    print(f\"Rating {rating} -> {converted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "258a3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved converted dataframe to 'df2_with_sentiment.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert ratings to one-hot encoded format\n",
    "df2['authenticity'] = df2['rating'].apply(rating_to_one_hot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a6e1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'video_no', 'sentence_no', 'split_video_no', 'text',\n",
      "       'start_time', 'end_time', 'offensive', 'offensiveness level',\n",
      "       'sentiment', 'video_path', 'audio_path'],\n",
      "      dtype='object')\n",
      "Index(['v_name', 'rating', 'sentiment', 'authenticity'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f0bbd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      video_no                                               text  \\\n",
      "0  video_1.mp4            what are your strengths and weaknesses?   \n",
      "1  video_1.mp4             sir, yeh sabse chutiya sawaal hai, sir   \n",
      "2  video_1.mp4                sabse pehle toh strength hai hi nhi   \n",
      "3  video_1.mp4                                 in english, please   \n",
      "4  video_1.mp4  I'm telling you. Why will I tell you my weakne...   \n",
      "\n",
      "                          video_path                             audio_path  \n",
      "0  ./final_data/videos/video_1_1.mp4  ./final_data/audio_conv/video_1_1.npy  \n",
      "1  ./final_data/videos/video_1_2.mp4  ./final_data/audio_conv/video_1_2.npy  \n",
      "2  ./final_data/videos/video_1_3.mp4  ./final_data/audio_conv/video_1_3.npy  \n",
      "3  ./final_data/videos/video_1_4.mp4  ./final_data/audio_conv/video_1_4.npy  \n",
      "4  ./final_data/videos/video_1_5.mp4  ./final_data/audio_conv/video_1_5.npy  \n"
     ]
    }
   ],
   "source": [
    "#print(df1[[\"text\"]].head())\n",
    "print(df1[[\"video_no\",\"text\",\"video_path\",\"audio_path\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d975803",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['offensive']=df2[\"sentiment\"]\n",
    "df2['offensiveness level']=df2[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e479cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"video_path\"]=\"./tiktok_data/video/\" + df2[\"v_name\"].astype(str) \n",
    "df2[\"audio_path\"] = \"./tiktok_data/audio_conv/\" + df2[\"v_name\"].str.replace(\".mp4\", \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "transcript_json=\"g_video/transcript.json\"\n",
    "with open (transcript_json, 'r') as file:\n",
    "    transcript_data = json.load(file)\n",
    "\n",
    "for v_id in transcript_data:\n",
    "    v_name=\"v_\"+v_id+\".mp4\"\n",
    "    if v_name in df2[\"v_name\"].values:\n",
    "        df2.loc[df2[\"v_name\"] == v_name, \"text\"] = transcript_data[v_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b75438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're using the correct whisper module\n",
    "import whisper\n",
    "import os\n",
    "import pandas as pd\n",
    "%cd ToxVidLM_ACL_2024/\n",
    "# Load Whisper model (using a smaller model for faster processing)\n",
    "print(\"Loading Whisper model...\")\n",
    "model = whisper.load_model(\"small\")  # You can use \"base\", \"small\", \"medium\", \"large\" based on your needs\n",
    "print(\"✅ Whisper model loaded successfully!\")\n",
    "\n",
    "# Function to transcribe a single video file\n",
    "def transcribe_video(video_path):\n",
    "    try:\n",
    "        print(f\"🎬 Transcribing: {video_path}\")\n",
    "        \n",
    "        # Whisper can directly process video files (extracts audio automatically)\n",
    "        result = model.transcribe(video_path)\n",
    "        \n",
    "        # Get the transcript text\n",
    "        transcript = result[\"text\"]\n",
    "        \n",
    "        print(f\"✅ Transcript: {transcript[:100]}...\" if len(transcript) > 100 else f\"✅ Transcript: {transcript}\")\n",
    "        return transcript\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transcribing {video_path}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff742fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process videos that don't have transcripts yet\n",
    "print(\"Processing videos without transcripts...\")\n",
    "\n",
    "transcribed_count = 0\n",
    "for i in range(len(df2)):\n",
    "    video_path = df2[\"video_path\"].iloc[i]\n",
    "    transcript = df2[\"text\"].iloc[i]\n",
    "    \n",
    "    # Check if transcript is missing or empty\n",
    "    if pd.isna(transcript) or transcript.strip() == \"\":\n",
    "        print(f\"\\n📹 Processing video {i+1}/{len(df2)}: {video_path}\")\n",
    "        \n",
    "        # Check if video file exists\n",
    "        if os.path.exists(video_path):\n",
    "            transcript = transcribe_video(video_path)\n",
    "            if transcript:\n",
    "                df2.at[i, \"text\"] = transcript\n",
    "                transcribed_count += 1\n",
    "            else:\n",
    "                print(f\"❌ Failed to transcribe {video_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Video file not found: {video_path}\")\n",
    "    else:\n",
    "        print(f\"✅ Video {i+1} already has transcript\")\n",
    "\n",
    "print(f\"\\n🎉 Transcription complete! Processed {transcribed_count} new videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6b4aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some transcript is nan\n",
    "df2['text'] = df2['text'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "964b1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"tiktok_data/video_rating.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052bbb6a",
   "metadata": {},
   "source": [
    "## Start training with our new config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9288703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 123\n",
      "tokenizer_config.json: 100%|█████████████████| 25.0/25.0 [00:00<00:00, 7.31kB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 2.96MB/s]\n",
      "config.json: 100%|██████████████████████████████| 481/481 [00:00<00:00, 284kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'XLMRobertaTokenizerFast'.\n",
      "You are using a model of type roberta to instantiate a model of type xlm-roberta. This is not supported for all configurations of models and can yield errors.\n",
      "model.safetensors: 100%|█████████████████████| 499M/499M [00:10<00:00, 46.8MB/s]\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [00:45<00:00,  3.50s/it]\n",
      "1.4794431191224318\n",
      "100%|███████████████████████████████████████████| 25/25 [01:17<00:00,  3.11s/it]\n",
      "sentiment   0.43   0.29580952380952374                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.43      1.00      0.60        43\n",
      "           4       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.43       100\n",
      "   macro avg       0.09      0.20      0.12       100\n",
      "weighted avg       0.18      0.43      0.26       100\n",
      " \n",
      "\n",
      "Epoch 2 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [00:48<00:00,  3.70s/it]\n",
      "1.310659335209773\n",
      "100%|███████████████████████████████████████████| 25/25 [01:18<00:00,  3.13s/it]\n",
      "sentiment   0.43   0.29580952380952374                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.43      1.00      0.60        43\n",
      "           4       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.43       100\n",
      "   macro avg       0.09      0.20      0.12       100\n",
      "weighted avg       0.18      0.43      0.26       100\n",
      " \n",
      "\n",
      "Epoch 3 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [00:44<00:00,  3.43s/it]\n",
      "1.285950834934528\n",
      "100%|███████████████████████████████████████████| 25/25 [01:18<00:00,  3.15s/it]\n",
      "sentiment   0.43   0.29580952380952374                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        18\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.43      1.00      0.60        43\n",
      "           4       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.43       100\n",
      "   macro avg       0.09      0.20      0.12       100\n",
      "weighted avg       0.18      0.43      0.26       100\n",
      " \n",
      "\n",
      "saved best model with metric:  f1  for task:  sentiment\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "!python train_tiktok.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef3bf9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "load state dict checkpoints/gpt2_vidmae_whisper_sentiment_0.pth\n",
      "100%|███████████████████████████████████████████| 17/17 [01:01<00:00,  3.63s/it]\n",
      "sentiment   0.35784313725490197   0.2274509803921569                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.00      0.00      0.00         9\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.36      1.00      0.53        24\n",
      "           4       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.36        67\n",
      "   macro avg       0.07      0.20      0.11        67\n",
      "weighted avg       0.13      0.36      0.19        67\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python test_tiktok.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c3b1a",
   "metadata": {},
   "source": [
    "# Try Engagement Rate Rather than Authenticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ed4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_name=\"tiktok_data/video_rating.csv\"\n",
    "df2_name=\"tiktok_data/dataset_450videos_2025stat_update.csv\"\n",
    "df1=pd.read_csv(df1_name)\n",
    "df2=pd.read_csv(df2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e4791ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v_name', 'url', 'play_count', 'user_follower_cnt',\n",
       "       'user_following_cnt', 'duration', 'comment_count', 'genre', 'type',\n",
       "       'sponsored', 'digg_count', 'median_rating', 'human_explanation',\n",
       "       'rating_desc_old', 'text_desc', 'influencer_gender',\n",
       "       '2025_comment_count', '2025_digg_count', 'engagement_rate',\n",
       "       'engagement_rate_2025'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f170fa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values: 0\n",
      "Non-null values: 450\n"
     ]
    }
   ],
   "source": [
    "nan_count = df2[\"engagement_rate_2025\"].isna().sum()\n",
    "print(f\"NaN values: {nan_count}\")\n",
    "print(f\"Non-null values: {len(df2) - nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34f5f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 CALCULATING ENGAGEMENT LEVELS\n",
      "==================================================\n",
      "📊 ENGAGEMENT LEVEL BOUNDARIES:\n",
      "   Level 1: 0.000000 - 0.029972\n",
      "   Level 2: 0.029972 - 0.053244\n",
      "   Level 3: 0.053244 - 0.084926\n",
      "   Level 4: 0.084926 - 0.116331\n",
      "   Level 5: 0.116331 - 0.328814\n",
      "\n",
      "📈 ENGAGEMENT LEVEL DISTRIBUTION:\n",
      "   Level 1: 90 videos (20.0%)\n",
      "   Level 2: 90 videos (20.0%)\n",
      "   Level 3: 94 videos (20.9%)\n",
      "   Level 4: 96 videos (21.3%)\n",
      "   Level 5: 80 videos (17.8%)\n",
      "\n",
      "📋 SAMPLE RESULTS:\n",
      "   Video 1: Rate=0.025931, Level=1, One-hot=[1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "   Video 2: Rate=0.097571, Level=4, One-hot=[0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "   Video 3: Rate=0.109244, Level=4, One-hot=[0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "   Video 4: Rate=0.103067, Level=4, One-hot=[0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "   Video 5: Rate=0.069315, Level=3, One-hot=[0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "   Video 6: Rate=0.039050, Level=2, One-hot=[0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "   Video 7: Rate=0.130128, Level=5, One-hot=[0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "   Video 8: Rate=0.084926, Level=3, One-hot=[0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "   Video 9: Rate=0.170303, Level=5, One-hot=[0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "   Video 10: Rate=0.112160, Level=4, One-hot=[0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "✅ Successfully created 'engagement_level' and 'engagement_one_hot' columns in df2!\n"
     ]
    }
   ],
   "source": [
    "# Calculate engagement level bins using quantile-based approach\n",
    "def calculate_engagement_levels(engagement_rates, n_levels=5):\n",
    "    \"\"\"\n",
    "    Calculate engagement levels (1-5) based on quantiles of engagement rates\n",
    "    \"\"\"\n",
    "    # Remove any NaN values for quantile calculation\n",
    "    valid_rates = engagement_rates.dropna()\n",
    "    \n",
    "    # Calculate quantile boundaries for 5 levels\n",
    "    quantiles = np.linspace(0, 1, n_levels + 1)\n",
    "    boundaries = valid_rates.quantile(quantiles).values\n",
    "    \n",
    "    print(f\"📊 ENGAGEMENT LEVEL BOUNDARIES:\")\n",
    "    for i in range(n_levels):\n",
    "        print(f\"   Level {i+1}: {boundaries[i]:.6f} - {boundaries[i+1]:.6f}\")\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "def engagement_to_level(rate, boundaries):\n",
    "    \"\"\"\n",
    "    Convert engagement rate to level (1-5)\n",
    "    \"\"\"\n",
    "    if pd.isna(rate):\n",
    "        return 3  # Default to middle level for NaN values\n",
    "    \n",
    "    for i in range(len(boundaries) - 1):\n",
    "        if rate <= boundaries[i + 1]:\n",
    "            return i + 1\n",
    "    return len(boundaries) - 1  # For values at the maximum\n",
    "\n",
    "def level_to_one_hot(level, num_classes=5):\n",
    "    \"\"\"\n",
    "    Convert engagement level to one-hot encoding\n",
    "    \"\"\"\n",
    "    one_hot = [0.0] * num_classes\n",
    "    if 1 <= level <= num_classes:\n",
    "        one_hot[level - 1] = 1.0\n",
    "    return str(one_hot)\n",
    "\n",
    "# Calculate engagement levels\n",
    "print(\"🔄 CALCULATING ENGAGEMENT LEVELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate boundaries based on engagement_rate_2025\n",
    "boundaries = calculate_engagement_levels(df2['engagement_rate_2025'])\n",
    "\n",
    "# Create engagement level column\n",
    "df2['engagement_level'] = df2['engagement_rate_2025'].apply(\n",
    "    lambda x: engagement_to_level(x, boundaries)\n",
    ")\n",
    "\n",
    "# Create one-hot encoded engagement level\n",
    "df2['engagement_one_hot'] = df2['engagement_level'].apply(level_to_one_hot)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n📈 ENGAGEMENT LEVEL DISTRIBUTION:\")\n",
    "level_counts = df2['engagement_level'].value_counts().sort_index()\n",
    "for level in range(1, 6):\n",
    "    count = level_counts.get(level, 0)\n",
    "    percentage = (count / len(df2)) * 100\n",
    "    print(f\"   Level {level}: {count} videos ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📋 SAMPLE RESULTS:\")\n",
    "sample_df = df2[['v_name', 'engagement_rate_2025', 'engagement_level', 'engagement_one_hot']].head(10)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"   Video {idx+1}: Rate={row['engagement_rate_2025']:.6f}, Level={row['engagement_level']}, One-hot={row['engagement_one_hot']}\")\n",
    "\n",
    "print(f\"\\n✅ Successfully created 'engagement_level' and 'engagement_one_hot' columns in df2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c37beee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2[[\"v_name\", 'engagement_level', \"engagement_one_hot\"]], on=\"v_name\", how=\"left\").to_csv(\"video_rating.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_tiktok.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a0cb4",
   "metadata": {},
   "source": [
    "## Try differnt parameters and see different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running test with random state: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 0\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [01:05<00:00,  5.06s/it]\n",
      "1.6115657824736376\n",
      "100%|███████████████████████████████████████████| 25/25 [01:14<00:00,  2.99s/it]\n",
      "sentiment   0.38   0.24342857142857136                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.38      1.00      0.55        38\n",
      "           4       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.38       100\n",
      "   macro avg       0.08      0.20      0.11       100\n",
      "weighted avg       0.14      0.38      0.21       100\n",
      " \n",
      "\n",
      "Epoch 2 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [01:08<00:00,  5.29s/it]\n",
      "1.4786499738693237\n",
      "100%|███████████████████████████████████████████| 25/25 [01:19<00:00,  3.16s/it]\n",
      "sentiment   0.38   0.2504285714285714                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.50      0.05      0.10        19\n",
      "           3       0.38      0.97      0.54        38\n",
      "           4       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.38       100\n",
      "   macro avg       0.18      0.21      0.13       100\n",
      "weighted avg       0.24      0.38      0.22       100\n",
      " \n",
      "\n",
      "Epoch 3 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [01:05<00:00,  5.03s/it]\n",
      "1.4442997070459218\n",
      "100%|███████████████████████████████████████████| 25/25 [01:20<00:00,  3.23s/it]\n",
      "sentiment   0.38   0.24342857142857136                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        19\n",
      "           3       0.38      1.00      0.55        38\n",
      "           4       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.38       100\n",
      "   macro avg       0.08      0.20      0.11       100\n",
      "weighted avg       0.14      0.38      0.21       100\n",
      " \n",
      "\n",
      "saved best model with metric:  f1  for task:  sentiment\n",
      "Training finished!\n",
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 0\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "load state dict checkpoints/gpt2_vidmae_whisper_sentiment_rd0_0.pth\n",
      "100%|███████████████████████████████████████████| 75/75 [03:22<00:00,  2.70s/it]\n",
      "sentiment   0.4033333333333333   0.28171428571428564                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00        48\n",
      "           2       0.00      0.00      0.00        51\n",
      "           3       0.40      1.00      0.57       121\n",
      "           4       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           0.40       300\n",
      "   macro avg       0.08      0.20      0.11       300\n",
      "weighted avg       0.16      0.40      0.23       300\n",
      " \n",
      "\n",
      "\n",
      "📊 DETAILED RESULTS SAVED TO: results/test_results_rd0.json\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT TASK RESULTS:\n",
      "   • Accuracy: 0.4033\n",
      "   • F1-Score: 0.2817\n",
      "   • MAE: 0.8233\n",
      "   • Per-class results:\n",
      "     - Class 0: 0/10 correct (0.00%)\n",
      "     - Class 1: 0/48 correct (0.00%)\n",
      "     - Class 2: 0/51 correct (0.00%)\n",
      "     - Class 3: 121/121 correct (100.00%)\n",
      "     - Class 4: 0/70 correct (0.00%)\n",
      "\n",
      "   • Confusion Matrix:\n",
      "     True\\Pred     0     1     2     3     4\n",
      "     Class 0:     0     0     0    10     0\n",
      "     Class 1:     0     0     0    48     0\n",
      "     Class 2:     0     0     0    51     0\n",
      "     Class 3:     0     0     0   121     0\n",
      "     Class 4:     0     0     0    70     0\n",
      "\n",
      "Running test with random state: 2\n",
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 2\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [00:55<00:00,  4.30s/it]\n",
      "1.582421871332022\n",
      "100%|███████████████████████████████████████████| 25/25 [01:24<00:00,  3.40s/it]\n",
      "sentiment   0.35   0.22685714285714276                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.35      1.00      0.52        35\n",
      "           4       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.35       100\n",
      "   macro avg       0.07      0.20      0.10       100\n",
      "weighted avg       0.12      0.35      0.18       100\n",
      " \n",
      "\n",
      "Epoch 2 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [01:08<00:00,  5.24s/it]\n",
      "1.3569321999183068\n",
      "100%|███████████████████████████████████████████| 25/25 [01:28<00:00,  3.54s/it]\n",
      "sentiment   0.35   0.22685714285714276                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.35      1.00      0.52        35\n",
      "           4       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.35       100\n",
      "   macro avg       0.07      0.20      0.10       100\n",
      "weighted avg       0.12      0.35      0.18       100\n",
      " \n",
      "\n",
      "Epoch 3 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [01:03<00:00,  4.85s/it]\n",
      "1.3070613696024969\n",
      "100%|███████████████████████████████████████████| 25/25 [01:25<00:00,  3.41s/it]\n",
      "sentiment   0.35   0.22685714285714276                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.35      1.00      0.52        35\n",
      "           4       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.35       100\n",
      "   macro avg       0.07      0.20      0.10       100\n",
      "weighted avg       0.12      0.35      0.18       100\n",
      " \n",
      "\n",
      "saved best model with metric:  f1  for task:  sentiment\n",
      "Training finished!\n",
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 2\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "load state dict checkpoints/gpt2_vidmae_whisper_sentiment_rd2_0.pth\n",
      "100%|███████████████████████████████████████████| 75/75 [03:21<00:00,  2.69s/it]\n",
      "sentiment   0.3933333333333333   0.2596825396825397                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.00      0.00      0.00        46\n",
      "           2       0.00      0.00      0.00        50\n",
      "           3       0.39      1.00      0.56       118\n",
      "           4       0.00      0.00      0.00        80\n",
      "\n",
      "    accuracy                           0.39       300\n",
      "   macro avg       0.08      0.20      0.11       300\n",
      "weighted avg       0.15      0.39      0.22       300\n",
      " \n",
      "\n",
      "\n",
      "📊 DETAILED RESULTS SAVED TO: results/test_results_rd2.json\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT TASK RESULTS:\n",
      "   • Accuracy: 0.3933\n",
      "   • F1-Score: 0.2597\n",
      "   • MAE: 0.8000\n",
      "   • Per-class results:\n",
      "     - Class 0: 0/6 correct (0.00%)\n",
      "     - Class 1: 0/46 correct (0.00%)\n",
      "     - Class 2: 0/50 correct (0.00%)\n",
      "     - Class 3: 118/118 correct (100.00%)\n",
      "     - Class 4: 0/80 correct (0.00%)\n",
      "\n",
      "   • Confusion Matrix:\n",
      "     True\\Pred     0     1     2     3     4\n",
      "     Class 0:     0     0     0     6     0\n",
      "     Class 1:     0     0     0    46     0\n",
      "     Class 2:     0     0     0    50     0\n",
      "     Class 3:     0     0     0   118     0\n",
      "     Class 4:     0     0     0    80     0\n",
      "\n",
      "Running test with random state: 3\n",
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 3\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [00:56<00:00,  4.36s/it]\n",
      "1.5782792293108427\n",
      "100%|███████████████████████████████████████████| 25/25 [01:23<00:00,  3.35s/it]\n",
      "sentiment   0.37   0.2455238095238094                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        22\n",
      "           3       0.37      1.00      0.54        37\n",
      "           4       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.37       100\n",
      "   macro avg       0.07      0.20      0.11       100\n",
      "weighted avg       0.14      0.37      0.20       100\n",
      " \n",
      "\n",
      "Epoch 2 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [01:01<00:00,  4.77s/it]\n",
      "1.4995078031833355\n",
      "100%|███████████████████████████████████████████| 25/25 [01:22<00:00,  3.31s/it]\n",
      "sentiment   0.23   0.20666666666666664                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        22\n",
      "           3       0.28      0.27      0.27        37\n",
      "           4       0.20      0.62      0.31        21\n",
      "\n",
      "    accuracy                           0.23       100\n",
      "   macro avg       0.10      0.18      0.12       100\n",
      "weighted avg       0.15      0.23      0.17       100\n",
      " \n",
      "\n",
      "Epoch 3 out of 3\n",
      "100%|███████████████████████████████████████████| 13/13 [01:00<00:00,  4.66s/it]\n",
      "1.4875794098927424\n",
      "100%|███████████████████████████████████████████| 25/25 [01:24<00:00,  3.38s/it]\n",
      "sentiment   0.37   0.2455238095238094                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        22\n",
      "           3       0.37      1.00      0.54        37\n",
      "           4       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.37       100\n",
      "   macro avg       0.07      0.20      0.11       100\n",
      "weighted avg       0.14      0.37      0.20       100\n",
      " \n",
      "\n",
      "saved best model with metric:  f1  for task:  sentiment\n",
      "Training finished!\n",
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 3\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "load state dict checkpoints/gpt2_vidmae_whisper_sentiment_rd3_0.pth\n",
      "100%|███████████████████████████████████████████| 75/75 [03:32<00:00,  2.84s/it]\n",
      "sentiment   0.41   0.2729523809523809                 precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.00      0.00      0.00        47\n",
      "           2       0.00      0.00      0.00        52\n",
      "           3       0.41      1.00      0.58       123\n",
      "           4       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           0.41       300\n",
      "   macro avg       0.08      0.20      0.12       300\n",
      "weighted avg       0.17      0.41      0.24       300\n",
      " \n",
      "\n",
      "\n",
      "📊 DETAILED RESULTS SAVED TO: results/test_results_rd3.json\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT TASK RESULTS:\n",
      "   • Accuracy: 0.4100\n",
      "   • F1-Score: 0.2730\n",
      "   • MAE: 0.8000\n",
      "   • Per-class results:\n",
      "     - Class 0: 0/8 correct (0.00%)\n",
      "     - Class 1: 0/47 correct (0.00%)\n",
      "     - Class 2: 0/52 correct (0.00%)\n",
      "     - Class 3: 123/123 correct (100.00%)\n",
      "     - Class 4: 0/70 correct (0.00%)\n",
      "\n",
      "   • Confusion Matrix:\n",
      "     True\\Pred     0     1     2     3     4\n",
      "     Class 0:     0     0     0     8     0\n",
      "     Class 1:     0     0     0    47     0\n",
      "     Class 2:     0     0     0    52     0\n",
      "     Class 3:     0     0     0   123     0\n",
      "     Class 4:     0     0     0    70     0\n",
      "\n",
      "Running test with random state: 4\n",
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 4\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at l3cube-pune/hing-roberta and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 out of 3\n",
      " 46%|████████████████████▎                       | 6/13 [00:44<00:29,  4.26s/it]^C\n",
      "/workspace/.conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/workspace/.conda/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "Using random state: 4\n"
     ]
    }
   ],
   "source": [
    "# Test different random states and compare results\n",
    "for i in range(100):\n",
    "    if i==1:\n",
    "        continue\n",
    "    print(f\"\\nRunning test with random state: {i}\")\n",
    "    !python train_tiktok.py --rd_state $i\n",
    "    !python test_tiktok.py --rd_state $i --save_predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results programmatically\n",
    "\n",
    "import json\n",
    "with open('results/test_results_rd1.json') as f: r1 = json.load(f)\n",
    "with open('results/test_results_rd100.json') as f: r100 = json.load(f)\n",
    "print('RD=1 Accuracy:', r1['test_metrics']['sentiment']['accuracy'])\n",
    "print('RD=100 Accuracy:', r100['test_metrics']['sentiment']['accuracy'])\n",
    "print('RD=1 MAE:', r1['detailed_analysis']['sentiment']['mae'])\n",
    "print('RD=100 MAE:', r100['detailed_analysis']['sentiment']['mae'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
