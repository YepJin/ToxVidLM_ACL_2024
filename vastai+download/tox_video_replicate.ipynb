{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3546bb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ToxVidLLM_ACL_2024\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5834864",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7ac5540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_final_data-20250630T215505Z-1-001.zip\n",
      "classification_final_data-20250630T215505Z-1-002.zip\n",
      "classification_final_data-20250630T215505Z-1-003.zip\n"
     ]
    }
   ],
   "source": [
    "!cd downloaded_data && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49dca176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting downloaded_data/classification_final_data-20250630T215505Z-1-001.zip...\n",
      "Extracting downloaded_data/classification_final_data-20250630T215505Z-1-002.zip...\n",
      "Extracting downloaded_data/classification_final_data-20250630T215505Z-1-002.zip...\n",
      "Extracting downloaded_data/classification_final_data-20250630T215505Z-1-003.zip...\n",
      "Extracting downloaded_data/classification_final_data-20250630T215505Z-1-003.zip...\n",
      "Extraction complete.\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "download_dir = \"downloaded_data\"\n",
    "# List all files in the download directory\n",
    "for filename in os.listdir(download_dir):\n",
    "    if filename.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(download_dir, filename)\n",
    "        print(f\"Extracting {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_dir)      \n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18b0b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_final_data\n",
      "classification_final_data-20250630T215505Z-1-001.zip\n",
      "classification_final_data-20250630T215505Z-1-002.zip\n",
      "classification_final_data-20250630T215505Z-1-003.zip\n"
     ]
    }
   ],
   "source": [
    "!ls downloaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c5cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv downloaded_data/classification_final_data final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89560f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1821298",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch==2.0.0 \n",
    "torchvision==0.15.1 \n",
    "torchaudio==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "694c0c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  2 02:30:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   0  Tesla V100-PCIE-16GB           On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   1  Tesla V100-PCIE-16GB           On  |   00000000:08:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE-16GB           On  |   00000000:09:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE-16GB           On  |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c2fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.12/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.12/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bab9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorchvideo==0.1.5 in ./.venv/lib/python3.12/site-packages (0.1.5)\n",
      "Requirement already satisfied: fvcore in ./.venv/lib/python3.12/site-packages (from pytorchvideo==0.1.5) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in ./.venv/lib/python3.12/site-packages (from pytorchvideo==0.1.5) (14.4.0)\n",
      "Requirement already satisfied: parameterized in ./.venv/lib/python3.12/site-packages (from pytorchvideo==0.1.5) (0.9.0)\n",
      "Requirement already satisfied: iopath in ./.venv/lib/python3.12/site-packages (from pytorchvideo==0.1.5) (0.1.10)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from pytorchvideo==0.1.5) (3.5)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from fvcore->pytorchvideo==0.1.5) (2.2.6)\n",
      "Requirement already satisfied: yacs>=0.1.6 in ./.venv/lib/python3.12/site-packages (from fvcore->pytorchvideo==0.1.5) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from fvcore->pytorchvideo==0.1.5) (6.0.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from fvcore->pytorchvideo==0.1.5) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in ./.venv/lib/python3.12/site-packages (from fvcore->pytorchvideo==0.1.5) (3.1.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from fvcore->pytorchvideo==0.1.5) (11.2.1)\n",
      "Requirement already satisfied: tabulate in ./.venv/lib/python3.12/site-packages (from fvcore->pytorchvideo==0.1.5) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions in ./.venv/lib/python3.12/site-packages (from iopath->pytorchvideo==0.1.5) (4.14.0)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.12/site-packages (from iopath->pytorchvideo==0.1.5) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ab0a65",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Normalize\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AugMix  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CutMix, MixUp, MixVideo  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrand_augment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandAugment  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmix.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     _AUGMENTATION_MAX_LEVEL,\n\u001b[32m      8\u001b[39m     AugmentTransform,\n\u001b[32m      9\u001b[39m     _decreasing_int_to_arg,\n\u001b[32m     10\u001b[39m     _decreasing_to_arg,\n\u001b[32m     11\u001b[39m     _increasing_magnitude_to_arg,\n\u001b[32m     12\u001b[39m     _increasing_randomly_negate_to_arg,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpSampler\n\u001b[32m     17\u001b[39m _AUGMIX_LEVEL_TO_ARG = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAutoContrast\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEqualize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAdjustSharpness\u001b[39m\u001b[33m\"\u001b[39m: _increasing_magnitude_to_arg,\n\u001b[32m     31\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmentations.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Optional, Tuple\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF_t\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroi_align\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[39m, in \u001b[36mregister_meta.<locals>.wrapper\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(fn):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension\u001b[49m._has_ops():\n\u001b[32m     19\u001b[39m         get_meta_lib().impl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch.ops.torchvision, op_name), overload_name), fn)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a35419",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     ApplyTransformToKey,\n\u001b[32m      3\u001b[39m     Normalize,\n\u001b[32m      4\u001b[39m     RandomShortSideScale,\n\u001b[32m      5\u001b[39m     RemoveKey,\n\u001b[32m      6\u001b[39m     ShortSideScale,\n\u001b[32m      7\u001b[39m     UniformTemporalSubsample,\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AugMix  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CutMix, MixUp, MixVideo  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrand_augment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandAugment  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmix.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     _AUGMENTATION_MAX_LEVEL,\n\u001b[32m      8\u001b[39m     AugmentTransform,\n\u001b[32m      9\u001b[39m     _decreasing_int_to_arg,\n\u001b[32m     10\u001b[39m     _decreasing_to_arg,\n\u001b[32m     11\u001b[39m     _increasing_magnitude_to_arg,\n\u001b[32m     12\u001b[39m     _increasing_randomly_negate_to_arg,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpSampler\n\u001b[32m     17\u001b[39m _AUGMIX_LEVEL_TO_ARG = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAutoContrast\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEqualize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAdjustSharpness\u001b[39m\u001b[33m\"\u001b[39m: _increasing_magnitude_to_arg,\n\u001b[32m     31\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmentations.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF_t\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Maximum global magnitude used for video augmentation.\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e370049b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Multimodal_LLM\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/model/model.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madditional_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM_fc, Conv1d_fc, FC_head, Gate_Attention\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01margparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Namespace \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomDataset\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/data/dataset.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m return_audio_tensor, return_image_tensor, return_video_tensor, prepare_batch\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m literal_eval\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     ApplyTransformToKey,\n\u001b[32m     13\u001b[39m     Normalize,\n\u001b[32m     14\u001b[39m     RandomShortSideScale,\n\u001b[32m     15\u001b[39m     RemoveKey,\n\u001b[32m     16\u001b[39m     ShortSideScale,\n\u001b[32m     17\u001b[39m     UniformTemporalSubsample,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     Compose,\n\u001b[32m     21\u001b[39m     Lambda,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     RandomRotation,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCustomDataset\u001b[39;00m(Dataset):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AugMix  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CutMix, MixUp, MixVideo  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrand_augment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandAugment  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmix.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     _AUGMENTATION_MAX_LEVEL,\n\u001b[32m      8\u001b[39m     AugmentTransform,\n\u001b[32m      9\u001b[39m     _decreasing_int_to_arg,\n\u001b[32m     10\u001b[39m     _decreasing_to_arg,\n\u001b[32m     11\u001b[39m     _increasing_magnitude_to_arg,\n\u001b[32m     12\u001b[39m     _increasing_randomly_negate_to_arg,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpSampler\n\u001b[32m     17\u001b[39m _AUGMIX_LEVEL_TO_ARG = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAutoContrast\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEqualize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAdjustSharpness\u001b[39m\u001b[33m\"\u001b[39m: _increasing_magnitude_to_arg,\n\u001b[32m     31\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmentations.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF_t\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Maximum global magnitude used for video augmentation.\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "from model.model import Multimodal_LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49389205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\;'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\;'\n",
      "/tmp/ipykernel_26235/4180570474.py:1: SyntaxWarning: invalid escape sequence '\\;'\n",
      "  '''\n",
      "/root/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "/tmp/ipykernel_26235/4180570474.py:1: SyntaxWarning: invalid escape sequence '\\;'\n",
      "  '''\n",
      "/tmp/ipykernel_26235/4180570474.py:1: SyntaxWarning: invalid escape sequence '\\;'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.transforms.functional_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madditional_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM_fc, FC_head, Gate_Attention\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01margparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Namespace \n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Multimodal_LLM\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomDataset\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01miteration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model, train_one_epoch, validate\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/model/model.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madditional_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM_fc, Conv1d_fc, FC_head, Gate_Attention\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01margparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Namespace \n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomDataset\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/data/dataset.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m return_audio_tensor, return_image_tensor, return_video_tensor, prepare_batch\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m literal_eval\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     ApplyTransformToKey,\n\u001b[32m     13\u001b[39m     Normalize,\n\u001b[32m     14\u001b[39m     RandomShortSideScale,\n\u001b[32m     15\u001b[39m     RemoveKey,\n\u001b[32m     16\u001b[39m     ShortSideScale,\n\u001b[32m     17\u001b[39m     UniformTemporalSubsample,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     Compose,\n\u001b[32m     21\u001b[39m     Lambda,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     RandomRotation,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCustomDataset\u001b[39;00m(Dataset):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AugMix  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CutMix, MixUp, MixVideo  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrand_augment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandAugment  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmix.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Optional\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     _AUGMENTATION_MAX_LEVEL,\n\u001b[32m      8\u001b[39m     AugmentTransform,\n\u001b[32m      9\u001b[39m     _decreasing_int_to_arg,\n\u001b[32m     10\u001b[39m     _decreasing_to_arg,\n\u001b[32m     11\u001b[39m     _increasing_magnitude_to_arg,\n\u001b[32m     12\u001b[39m     _increasing_randomly_negate_to_arg,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorchvideo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpSampler\n\u001b[32m     17\u001b[39m _AUGMIX_LEVEL_TO_ARG = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAutoContrast\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEqualize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAdjustSharpness\u001b[39m\u001b[33m\"\u001b[39m: _increasing_magnitude_to_arg,\n\u001b[32m     31\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ToxVidLLM_ACL_2024/.venv/lib/python3.12/site-packages/pytorchvideo/transforms/augmentations.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF_t\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Maximum global magnitude used for video augmentation.\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision.transforms.functional_tensor'"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118torchvision torchaudio\n",
    "python -W ignore run.py\n",
    "python block_core.py\n",
    "find -type d -name 'pymp*' -exec rm -r {} \\;\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import AddedToken\n",
    "from transformers import CLIPModel, VideoMAEModel, Wav2Vec2Model, VideoMAEConfig, CLIPConfig, Wav2Vec2Config, XLMRobertaConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
    "from model.additional_modules import LSTM_fc, FC_head, Gate_Attention\n",
    "from argparse import Namespace \n",
    "from model.model import Multimodal_LLM\n",
    "from data.dataset import CustomDataset\n",
    "from iteration import train_model, train_one_epoch, validate\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, AlbertTokenizer, XLMRobertaTokenizerFast, PreTrainedTokenizerFast #only for gpt2 and assign values\n",
    "from transformers import GPT2Model, BertModel, AlbertModel, XLMRobertaModel\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_bool = {\"offensive\" : True, \"offensive_level\": True, \"sentiment\" : True}\n",
    "tasks = []\n",
    "name = \"gpt2_vidmae_whisper_\"\n",
    "\n",
    "for k, v in tasks_bool.items():\n",
    "    if tasks_bool[k]:\n",
    "        tasks.append(k)\n",
    "        name += k + \"_\"\n",
    "        \n",
    "config = Namespace(\n",
    "    file_name=name + \"0\",\n",
    "    device=torch.device(\"cuda:1\"),\n",
    "    tokenizer_path=\"ckpts\",\n",
    "    tasks = tasks,\n",
    "    offensive_bool = tasks_bool[\"offensive\"],\n",
    "    offensive_level_bool = tasks_bool[\"offensive_level\"],\n",
    "    sentiment_bool = tasks_bool[\"sentiment\"],\n",
    "    video_encoder=\"MCG-NJU/videomae-base\",\n",
    "    audio_encoder=\"openai/whisper-small\",\n",
    "    lstm_or_conv = False,\n",
    "    image_conv_kernel=23,\n",
    "    image_conv_stride=3,\n",
    "    image_conv_padding=8,\n",
    "    video_conv_kernel=36,\n",
    "    video_conv_stride=24,\n",
    "    video_conv_padding=0,\n",
    "    audio_conv_kernel=50,\n",
    "    audio_conv_stride=23,\n",
    "    audio_conv_padding=1,\n",
    "    llm_embed_dim=768,\n",
    "    llm_output_dim=768,\n",
    "    attn_dropout=0.1,\n",
    "    is_add_bias_kv=True,\n",
    "    is_add_zero_attn=True,\n",
    "    attention_heads=8,\n",
    "    image_dim=768,\n",
    "    video_dim=768,\n",
    "    audio_dim=768,\n",
    "    image_seq_len=197,\n",
    "    video_seq_len=1568,\n",
    "    audio_seq_len=1500,\n",
    "    min_mm_seq_len=64,\n",
    "    lstm_num_layers=1,\n",
    "    tokenizer_max_len=128,\n",
    "    add_pooling = False,\n",
    "    train=True,\n",
    "    directory = \"checkpoints/\",\n",
    "    results_directory = \"results/\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"final_data/final_processed_data_one_hot.csv\")\n",
    "df_train_val, df_test = train_test_split(df, test_size=0.1, random_state=28703)\n",
    "df_train, df_val = train_test_split(df_train_val, test_size=0.1, random_state=28703)\n",
    "\n",
    "num_epochs = 30\n",
    "patience = 10\n",
    "batch_size = 2\n",
    "\n",
    "#for roberta\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"l3cube-pune/hing-roberta\")\n",
    "model = XLMRobertaModel.from_pretrained(\"l3cube-pune/hing-roberta\", torch_dtype=torch.float32)\n",
    "\n",
    "#for gpt2\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained('l3cube-pune/hing-gpt')\n",
    "# model = GPT2Model.from_pretrained('l3cube-pune/hing-gpt', torch_dtype=torch.float32)\n",
    "# tokenizer.bos_token_id = 1\n",
    "# tokenizer.eos_token_id = 2\n",
    "\n",
    "\n",
    "model = Multimodal_LLM(batch_size=batch_size, config=config, tokenizer=tokenizer, adapter_llm=model)\n",
    "\n",
    "train_ds = CustomDataset(dataframe=df_train, train=True, tokenizer=tokenizer)\n",
    "val_ds = CustomDataset(df_val, train=True, tokenizer=tokenizer)\n",
    "test_ds = CustomDataset(df_test, train=False, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=batch_size, num_workers=16)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, num_workers=16)\n",
    "\n",
    "\n",
    "train_model(model, train_dataloader, val_dataloader, config, num_epochs, \"offensive\", \"f1\", devices=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
